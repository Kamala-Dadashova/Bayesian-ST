---
title: 
author: "Kamala Dadashova"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r, echo=FALSE,include=FALSE}
library(tibble)
library(ggplot2)
library(tidyr)
library(dplyr)
library(invgamma)
library(ggforce)
library(rjags)
```


**Problem 1**

Fit the following model to the NBA free throw data:

|Player|Overall proportion| Clutch makes|  Clutch attempts|
|:-----------------------------|:--------------------|:----------------------|:-----------------------|
|Russell Westbrook| 0.845| 64| 75|
|James Harden| 0.847| 72| 95|
|Kawhi Leonard| 0.880 | 55 | 63|
|LeBron James| 0.674 | 27 | 39|
|Isaiah Thomas| 0.909 | 75 | 83|
|Stephen Curry| 0.898 | 24 | 26|
|Giannis Antetokounmpo| 0.770 | 28 | 41|
|John Wall|  0.801 | 66 | 82|
|Anthony Davis| 0.802 | 40 | 54|
|Kevin Durant| 0.875 | 13 | 16

$Y_i|\theta_i \sim Binomial(n_i; \theta_i)$ and $\theta_i|m \sim  Beta[exp(m)q_i, exp(m)(1-q_i)],$
where $Y_i$ is the number of made clutch shots for player $i = 1,..., 10,$
$n_i$ is the number of attempted clutch shots, $q_i \in (0, 1)$ is the overall
proportion, and $m  \sim Normal(0, 10)$.


a) Why this is a reasonable prior for $\theta_i$.

Since the domain of the beta distribution covers 0 to 1 and the mean of the beta distribution is $$\frac{e^m q_i}{e^m q_i + e^m (1-q_i) }=  q_i $$ so distribution is centered at $q_i.$ and takes values on [0,1].


b) What is the role of $m$ in the prior.

This determines the spread of the distribution around $q_i$.

c) Derive the full conditional posterior for $\theta_1$.

\begin{align*} f(\theta_1|Y_1,...,Y_{10},\theta_2,...\theta_{10},m) & =\frac{f(Y_1,...,Y_{10}, \theta_1, ...,\theta_{10},m)}{f(Y_1,...,Y_{10}, \theta_2, ...,\theta_{10},m)}\\ &\propto f(Y_1,...,Y_{10}, \theta_1, ...,\theta_{10},m) \\&\propto f(Y_1,...,Y_{10}|\theta_1, ...,\theta_{10},m) f(\theta_1, ...,\theta_{10},m)\\ &\propto f(Y_1,...,Y_{10}|\theta_1, ...,\theta_{10},m) f(\theta_1, ...,\theta_{10}|m) f(m)\\ &\propto f(Y_1,...,Y_{10}|\theta_1, ...,\theta_{10}) f(\theta_1, ...,\theta_{10}|m) \\ &\propto \prod_{i=1}^{10} f(Y_i|\theta_i)f(\theta_i|m)\\& \propto f(Y_1|\theta_1)f(\theta_1|m)  \\ &\propto Beta(Y_1 + exp(m)q_1, n_1-Y_1 + exp(m)(1-q_1))\end{align*}

d) Write your own MCMC algorithm to compute a table of posterior
means and 95% credible intervals for all 11 model parameters
$(\theta_1,...,\theta_{10},m)$. 

Similarly as previous part we obtain the full conditional posterior of each $\theta_i,$ $$\theta_i|rest \propto Beta(Y_i + exp(m)q_i, n_i-Y_i + exp(m)(1-q_i))  \qquad i = 1,...,10.$$ However,  we don't have nice form of the full conditional distribution for $m$,so we combine Gibbs and Metropolis algorithm.


```{r Gibbs + Metropolis}
set.seed(100)
#given values in table
Y = c(64,72,55,27,75,24,28,66,40,13)
n = c(75,95,63,39,83,26,41,82,54,16)
q = c(0.845, 0.847, 0.880, 0.674, 0.909, 0.898, 0.770, 0.801, 0.802, 0.875)
N=10
#parameters to start MCMC
m = 0;
theta = q; 
iters = 3*10^4;
burn= 10^4; 
MCMC=matrix(0,iters-burn,N+1); # save only after burn-in
can_sd = 1
#log posterior for m
log_post_m = function(theta, Y, n, q, m){
  like = 0
  for(i in 1:10){
  like = like + dbeta(theta[i],exp(m)*q[i],exp(m)*(1-q[i]),log = TRUE )
  }
  prior = dnorm(m,0,sqrt(10),log=TRUE)
  return(like + prior)}

for(iter in 1:iters){
   # Gibbs for each theta
  for(i in 1:N){
    alpha = Y[i] + exp(m) * q[i]
    beta = n[i] - Y[i] + exp(m) * (1 - q[i])
    theta[i] = rbeta(1, alpha, beta)
  }
  # Metropolis for m
  can = rnorm(1, m, can_sd) #proposal distribution
  logR = log_post_m(theta, Y, n, q, can) - log_post_m(theta, Y, n, q, m) 
  R=exp(logR)
  if(runif(1) < R){m = can}
  if(iter>burn){ 
  MCMC[iter - burn, ] = c(theta,m)}
}

acc_rate = mean(MCMC[2:(iters-burn),11]!=MCMC[1:(iters-burn-1),11])
MCMC=data.frame(MCMC) 
colnames(MCMC)=c("Russell Westbrook","James Harden","Kawhi Leonard","LeBron James",
"Isaiah Thomos","Stephen Curry","Giannis Antetokounmpo","John Wall",
"Anthony Davis","Kevin Durant","m")

```

The trace plots of MCMC sample of parameters are presented below.


```{r trace_plots}
MCMC%>%
pivot_longer(cols = "Russell Westbrook":"m", names_to = "Parameters", 
             values_to = "Posterior_Distributions")%>%
  ggplot(aes(x=rep(seq(burn+1,iters), 11), y = Posterior_Distributions))+
  xlab("Iterations")+
  geom_line(size=.1)+theme(plot.title = element_text(hjust = 0.5))+
  ggtitle(" Trace plot of the MCMC samples of each posteriors")+
  facet_wrap_paginate(~Parameters,scales = "free", ncol = 2, nrow = 2, page = 1)

MCMC%>%
pivot_longer(cols = "Russell Westbrook":"m", names_to = "Parameters", 
             values_to = "Posterior_Distributions")%>%
  ggplot(aes(x=rep(seq(burn+1,iters), 11), y = Posterior_Distributions))+
  xlab("Iterations")+
  geom_line(size=.1)+theme(plot.title = element_text(hjust = 0.5))+
  ggtitle(" Trace plot of the MCMC samples of each posteriors")+
  facet_wrap_paginate(~Parameters,scales = "free", ncol = 2, nrow = 2, page = 2)

MCMC%>%
pivot_longer(cols = "Russell Westbrook":"m", names_to = "Parameters", 
             values_to = "Posterior_Distributions")%>%
  ggplot(aes(x=rep(seq(burn+1,iters), 11), y = Posterior_Distributions))+
  xlab("Iterations")+
  geom_line(size=.1)+theme(plot.title = element_text(hjust = 0.5))+
  ggtitle(" Trace plot of the MCMC samples of each posteriors")+
  facet_wrap_paginate(~Parameters,scales = "free", ncol = 2, nrow = 2, page = 3)
```
```{r,echo=TRUE,out.width="50%"}
plot(seq(burn+1,iters),MCMC[ ,11],type="l",xlab="Iteration",ylab="Sample for m",
main=paste("Acceptance prob =",round(acc_rate,2)))
```



```{r tables}
table=sapply(MCMC, quantile,  probs = c(.5, 0.025, 0.975))
rownames(table) = c("Means","2.5 %","97.5 %") 
knitr::kable(t(table))
```

e)  Fit the same model in JAGS. Turn in commented code, and
comment on whether the two algorithms returned the same
results.

```{r}
#given data
Y = c(64,72,55,27,75,24,28,66,40,13)
n = c(75,95,63,39,83,26,41,82,54,16)
q= c(0.845, 0.847, 0.880, 0.674, 0.909, 0.898, 0.770, 0.801, 0.802, 0.875)
N = 10
# define string model
model_string = textConnection("model{
   # Likelihood
    for(i in 1:N){
      Y[i] ~ dbin(theta[i], n[i])
    }
   # Priors
    for(i in 1:N){
      theta[i] ~ dbeta(exp(m)*q[i], exp(m)*(1-q[i]))
    }
    
    m   ~  dnorm(0, 0.1)
 }")
# Load the data and compile the MCMC code
 inits = list(theta=q, m = 0)
 data  = list(Y = Y, n = n, q = q, N= N)
 model = jags.model(model_string,data = data, inits=inits, n.chains=2)
 #Burn-in for 10000 samples
update(model, 10000, progress.bar="none")
# Generate 20000 post-burn-in samples


```


```{r}
 params  = c("theta","m")
 samples = coda.samples(model, 
            variable.names=params, 
            n.iter=20000, progress.bar="none")
```

```{r}
 summary(samples)
```

```{r,echo=TRUE,out.width="50%"}
plot(samples)
```


f)  What are the advantages and disadvantages of writing your own
code as opposed to using JAGS in this problem and in general?

The benefit of doing your own coding is that you have more control over the algorithm; the drawbacks are that it often takes longer and is more error-prone.
