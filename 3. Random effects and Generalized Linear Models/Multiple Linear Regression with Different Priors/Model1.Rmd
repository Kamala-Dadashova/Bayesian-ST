---
title: 
output: pdf_document
---

\section{Multiple linear regression for the HOMES data}

We use the data from Barberan (2015), downloaded from http://figshare.com/articles/1000homes/1270900. The data are dust samples from the ledges above doorways from n=1,059 homes (after removing samples with missing data) in the continental US. Bioinformatics processing detects the presence or absence of 763 species (technically operational taxonomic units) of fungi. The response is the log of the number of fungi species present in the sample, which is a measure of species richness. The objective is to determine which factors influence a home's species richness. For each home, eight covariates are included in this example: longitude, latitude, annual mean temperature, annual mean precipitation, net primary productivity (NPP), elevation, the binary indicator that the house is a single-family home, and the number of bedrooms in the home. These covariates are all centered and scaled to have mean zero and variance one.

The Bayesian multiple linear regression model is
$$Y_i\sim\mbox{Normal}(\beta_0+\sum_{j=1}^px_{ij}\beta_j,\sigma^2).$$
Below we compare three different prior for the slopes $\beta_1,â€¦,\beta_p$

1. Uninformative Gaussian: $\beta_j\sim\mbox{Normal}(0,1000)$

2. Gaussian shrinkage: $\beta_j\sim\mbox{Normal}(0,\sigma_b^2)$ with $\sigma_b^2\sim\mbox{InvGamma}(0.1,0.1)$

3. Bayesian LASSO: $\beta_j\sim\mbox{DE}(0,\sigma_b^2)$ with $\sigma_b^2\sim\mbox{InvGamma}(0.1,0.1)$

In all cases, we use uninformative conjugate priors for the intercept $\beta_0\sim\mbox{Normal}(0,1000)$ and variance $\sigma^2\sim\mbox{InvGamma}(0.1,0.1)$

\section{Load and plot the data}
```{r,echo=FALSE}
library(rjags)
 # Load data
load("/Users/kamaladadashova/Documents/Ph.D. Courses/APPLIED BAYESIAN /Bayesian lecture/Week 6 Linear Models/homes.RData")
```

```{r}
lat      <- homes[,4]
long     <- homes[,5]
temp     <- homes[,6]
precip   <- homes[,7]
NPP      <- homes[,8]
elev     <- homes[,9]
house    <- ifelse(homes[,10]=="One-family house detached from any other house",1,0)
bedrooms <- as.numeric(homes[,11])
```

```{r}
city     <- homes[,2]
  state    <- homes[,3]

  OTU      <- as.matrix(OTU)
  nspecies <- rowSums(OTU>0)
  Y        <- log(nspecies)
  X        <- cbind(long,lat,temp,precip,NPP,elev,house,bedrooms)
  names    <- c("Longitude","Latitude",
                "Temperature","Precipitation","NPP",
                "Elevation","Single-family home",
                "Number of bedrooms")

 # Remove observations with missing data

  junk     <- is.na(rowSums(X))
  Y        <- Y[!junk]
  X        <- X[!junk,]
  city     <- city[!junk]
  state    <- state[!junk]

 # Standardize the covariates

  X        <- as.matrix(scale(X))

 # Plot the sample locations

 library(maps)
 map("state")
 points(homes[,5],homes[,4],pch=19,cex=.5)
 title("Sample locations")
```


\section{Put the data in JAGS format}
```{r}
 n        <- length(Y)
 p        <- ncol(X)

 data   <- list(Y=Y,X=X,n=n,p=p)
 params <- c("beta")

 burn     <- 10000
 n.iter   <- 20000
 thin     <- 10
 n.chains <- 2
```

\section{(1) Fit the uninformative Gaussian model}
```{r}
 model_string <- textConnection("model{
   # Likelihood
    for(i in 1:n){
      Y[i] ~ dnorm(alpha+inprod(X[i,],beta[]),taue)
    }
   # Priors
    for(j in 1:p){
      beta[j] ~ dnorm(0,0.001)
    }
    alpha ~ dnorm(0,0.001)
    taue  ~ dgamma(0.1, 0.1)
 }")

 model <- jags.model(model_string,data = data, n.chains=n.chains,quiet=TRUE)
 update(model, burn, progress.bar="none")
 samples1 <- coda.samples(model, variable.names=params, thin=thin, n.iter=n.iter, progress.bar="none")
par(mar=c(2,2,2,2))
plot(samples1)
```

```{r}
 round(effectiveSize(samples1),1)
```

```{r}
 sum                      <- summary(samples1)
 rownames(sum$statistics) <- names
 rownames(sum$quantiles)  <- names
 sum$statistics           <- round(sum$statistics,3)
 sum$quantiles            <- round(sum$quantiles,3)
 sum
```

\section{(2) Fit the Gaussian shrinkage model}
```{r}
 model_string <- textConnection("model{
   # Likelihood
    for(i in 1:n){
      Y[i] ~ dnorm(alpha+inprod(X[i,],beta[]),taue)
    }
   # Priors
    for(j in 1:p){
      beta[j] ~ dnorm(0,taue*taub)
    }
    alpha ~ dnorm(0,0.001)
    taue  ~ dgamma(0.1, 0.1)
    taub  ~ dgamma(0.1, 0.1)
 }")

 model <- jags.model(model_string,data = data, n.chains=n.chains,quiet=TRUE)
 update(model, burn, progress.bar="none")
 samples2 <- coda.samples(model, variable.names=params, thin=thin, n.iter=n.iter, progress.bar="none")
par(mar=c(2,2,2,2))
 plot(samples2)
```

```{r}
round(effectiveSize(samples2),1)
```

```{r}
sum                      <- summary(samples2)
 rownames(sum$statistics) <- names
 rownames(sum$quantiles)  <- names
 sum$statistics           <- round(sum$statistics,3)
 sum$quantiles            <- round(sum$quantiles,3)
 sum

```

\section{(3) Fit the Bayesian LASSO model}
```{r}
 model_string <- textConnection("model{
   # Likelihood
    for(i in 1:n){
      Y[i] ~ dnorm(alpha+inprod(X[i,],beta[]),taue)
    }
   # Priors
    for(j in 1:p){
      beta[j] ~ ddexp(0,taue*taub)
    }
    alpha ~ dnorm(0,0.001)
    taue  ~ dgamma(0.1, 0.1)
    taub  ~ dgamma(0.1, 0.1)
 }")

 model <- jags.model(model_string,data = data, n.chains=n.chains,quiet=TRUE)
 update(model, burn, progress.bar="none")
 samples3 <- coda.samples(model, variable.names=params, thin=thin, n.iter=n.iter, progress.bar="none")
par(mar=c(2,2,2,2))
 plot(samples3)
```

```{r}
 round(effectiveSize(samples3),1)
```

```{r}
 sum                      <- summary(samples3)
 rownames(sum$statistics) <- names
 rownames(sum$quantiles)  <- names
 sum$statistics           <- round(sum$statistics,3)
 sum$quantiles            <- round(sum$quantiles,3)
 sum
```

\section{Compare the three fits}

The plots below show the posterior for each covariate's slope for the three models:

1. Uninformative Gaussian is the solid line
2. Gaussian shrinkage is the dotted line
3. Bayesian LASSO is the dashed line

```{r,echo=TRUE,out.width="50%"}
for(j in 1:p){

 # Collect the MCMC iteration from both chains for the three priors

 s1 <- c(samples1[[1]][,j],samples1[[2]][,j])
 s2 <- c(samples2[[1]][,j],samples2[[2]][,j])
 s3 <- c(samples3[[1]][,j],samples3[[2]][,j])

 # Get smooth density estimate for each prior

 d1 <- density(s1)
 d2 <- density(s2)
 d3 <- density(s3)

 # Plot the density estimates

 mx <- max(c(d1$y,d2$y,d3$y))
par(mar=c(2,2,2,2))
 plot(d1$x,d1$y,type="l",ylim=c(0,mx),xlab=expression(beta),ylab="Posterior density",main=names[j])
 lines(d2$x,d2$y,lty=2)
 lines(d3$x,d3$y,lty=3)
 abline(v=0)
}
```

Since n is so much bigger than p, the prior has little effect on the posterior. In all three models temperature, NPP, elevation, and single-family home are the most important predictors.




