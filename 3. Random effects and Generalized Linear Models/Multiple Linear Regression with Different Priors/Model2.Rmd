---
title:
output: pdf_document
---

\section{Multiple linear regression prediction}

Let $Y_i$ be the percent increase in GOP support from 2012 to 2016 in county $i=1,…,n$. We model

$$Y_i|\beta,\sigma^2\sim \mbox{Normal}(\alpha+X_{1i}\beta_1+…+X_{pi}\beta_p,\sigma^2)$$

where $X_{ji}$ s the $j^{th}$ covariate for county $i$. All variables are centered and scaled. We select prior $\sigma^2\sim\mbox{InvGamma}(0.01,0.01)$ and $\alpha\sim\mbox{Normal}(0,100)$ for the error variance and intercept, and compare different priors for the regression coefficients. 

\section{Load and standardize the election data}

```{r,echo=FALSE}
load("/Users/kamaladadashova/Documents/Ph.D. Courses/APPLIED BAYESIAN /Bayesian lecture/Week 6 Linear Models/election_2008_2016.RData")
```
```{r}
# Load the data
 junk <- is.na(Y+rowSums(X))
 Y    <- Y[!junk]
 X    <- X[!junk,]
 n    <- length(Y)
 p    <- ncol(X)
 n
 p
```

```{r}
 X    <- scale(X)
# Fit the model to a training set of size 100 and make prediction for the remaining observations
 set.seed(0820)
 test  <- order(runif(n))>100
 table(test)
```

```{r}
 Yo    <- Y[!test]    # Observed data
 Xo    <- X[!test,]

 Yp    <- Y[test]     # Counties set aside for prediction
 Xp    <- X[test,]

 no    <- length(Yo)
 np    <- length(Yp)
 p     <- ncol(Xo)
```

\section{Fit the linear regression model with Gaussian priors}

```{r}
library(rjags)


model_string <- "model{

  # Likelihood
  for(i in 1:no){
    Yo[i]   ~ dnorm(muo[i],inv.var)
    muo[i] <- alpha + inprod(Xo[i,],beta[])
  }

  # Prediction
  for(i in 1:np){
    Yp[i]  ~ dnorm(mup[i],inv.var)
    mup[i] <- alpha + inprod(Xp[i,],beta[])
  }

  # Priors
  for(j in 1:p){
    beta[j] ~ dnorm(0,0.0001)
  }
  alpha     ~ dnorm(0, 0.01)
  inv.var   ~ dgamma(0.01, 0.01)
  sigma     <- 1/sqrt(inv.var)
}"
```

\section{Compile the model in JAGS}

```{r}
# NOTE: Yp is not sent to JAGS!
model <- jags.model(textConnection(model_string), 
                    data = list(Yo=Yo,no=no,np=np,p=p,Xo=Xo,Xp=Xp))
```

```{r}
update(model, 10000, progress.bar="none")

samp <- coda.samples(model, 
        variable.names=c("beta","sigma","Yp","alpha"), 
        n.iter=20000, progress.bar="none")

summary(samp[,-c(1:np)])
```

\section{Plot samples from the posterior preditive distribution (PPD) and plug-in distribution}

```{r,echo=TRUE,out.width="50%"}
#Extract the samples for each parameter

 samps       <- samp[[1]]
 Yp.samps    <- samps[,1:np] 
 alpha.samps <- samps[,np+1]
 beta.samps  <- samps[,np+1+1:p]
 sigma.samps <- samps[,ncol(samps)]

# Compute the posterior mean for the plug-in predictions  

 beta.mn  <- colMeans(beta.samps)
 sigma.mn <- mean(sigma.samps)
 alpha.mn <- mean(alpha.samps) 


# Plot the PPD and plug-in

 for(j in 1:5){

    # Plug-in
    mu <- alpha.mn+sum(Xp[j,]*beta.mn)
    y  <- rnorm(20000,mu,sigma.mn)
    plot(density(y),col=2,xlab="Y",main="PPD")

    # PPD
    lines(density(Yp.samps[,j]))

    # Truth
    abline(v=Yp[j],col=3,lwd=2)

    legend("topright",c("PPD","Plug-in","Truth"),col=1:3,lty=1,inset=0.05)
 }

```


```{r}
 # plug-in 95% intervals
  low1   <- alpha.mn+Xp%*%beta.mn - 1.96*sigma.mn
  high1  <- alpha.mn+Xp%*%beta.mn + 1.96*sigma.mn
  cover1 <- mean(Yp>low1 & Yp<high1)
  mean(cover1)
```

```{r}
 # PPD 95% intervals
  low2   <- apply(Yp.samps,2,quantile,0.025)
  high2  <- apply(Yp.samps,2,quantile,0.975)
  cover2 <- mean(Yp>low2 & Yp<high2)
  mean(cover2)
```

Notice how the PPD densities are slightly wider than the plug-in densities. This is the effect of accounting for uncertainty in $\beta$ and $\sigma$,  and it explains the slightly lower covarage for the plug-in predictions. However, for these data coverage is still OK for the plug-in predictions.
